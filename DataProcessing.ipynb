{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f26461f-7717-409f-9377-1accc258df76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-07-27 19:30:15 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-07-27 19:30:16 nemo_logging:349] /opt/conda/envs/sanas2/lib/python3.8/site-packages/whisper/timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "      def backtrace(trace: np.ndarray):\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"plbert/\")\n",
    "\n",
    "from nemo.collections.asr.models import EncDecSpeakerLabelModel\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as F\n",
    "import math\n",
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "import wave\n",
    "import sys\n",
    "import contextlib\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import yaml\n",
    "from transformers import AlbertConfig, AlbertModel\n",
    "\n",
    "from phonemize import phonemize\n",
    "from phonemizer.backend import EspeakBackend\n",
    "from transformers import TransfoXLTokenizer\n",
    "from text_normalize import normalize_text, remove_accents\n",
    "from text_utils import TextCleaner\n",
    "\n",
    "import whisper\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a9e3f1-51a5-40af-9679-c170cde8c501",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PL-BERT Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace258e1-d03f-40bd-afe6-599b1d52c663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertModel(\n",
       "  (embeddings): AlbertEmbeddings(\n",
       "    (word_embeddings): Embedding(178, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (encoder): AlbertTransformer(\n",
       "    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (albert_layer_groups): ModuleList(\n",
       "      (0): AlbertLayerGroup(\n",
       "        (albert_layers): ModuleList(\n",
       "          (0): AlbertLayer(\n",
       "            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (attention): AlbertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (attention_dropout): Dropout(p=0, inplace=False)\n",
       "              (output_dropout): Dropout(p=0, inplace=False)\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (ffn): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (ffn_output): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (activation): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (pooler_activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plbert_root = \"plbert/\"\n",
    "log_dir = plbert_root+\"Checkpoint/\"\n",
    "config_path = os.path.join(log_dir, \"config.yml\")\n",
    "plbert_config = yaml.safe_load(open(config_path))\n",
    "\n",
    "albert_base_configuration = AlbertConfig(**plbert_config['model_params'])\n",
    "plbert = AlbertModel(albert_base_configuration)\n",
    "\n",
    "files = os.listdir(log_dir)\n",
    "ckpts = []\n",
    "for f in os.listdir(log_dir):\n",
    "    if f.startswith(\"step_\"): ckpts.append(f)\n",
    "\n",
    "iters = [int(f.split('_')[-1].split('.')[0]) for f in ckpts if os.path.isfile(os.path.join(log_dir, f))]\n",
    "iters = sorted(iters)[-1]\n",
    "\n",
    "checkpoint = torch.load(log_dir + \"/step_\" + str(iters) + \".t7\", map_location='cpu')\n",
    "\n",
    "state_dict = checkpoint['net']\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    if name.startswith('encoder.'):\n",
    "        name = name[8:] # remove `encoder.`\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "plbert.load_state_dict(new_state_dict)\n",
    "plbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5a717f-8451-4cfd-a355-10618d9fbcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section will most likely be placed in collator function. With the batch of sentences, we need to pad them, and then pass it\n",
    "through the model.\n",
    "\"\"\"\n",
    "batch_of_text = [\"Hi, And also can you please check what is the current temperature setting of your unit both fridge and the freezer?\",\n",
    "                 \"Hi, do you mind returning the toy back to my house?\",\n",
    "                 \"Hi, I love the dress you are wearing!\", \n",
    "                 \"With the batch of sentences, we need to pad them, and then pass it through the model.\"]\n",
    "batch_of_text = batch_of_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec07d7d-b6c8-4005-a216-3fcdce93cde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "global_phonemizer = EspeakBackend(language='en-us', preserve_punctuation=True, with_stress=True) #make sure brew install espeak and export location of .dylib\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(plbert_config['dataset_params']['tokenizer'])\n",
    "text_cleaner = TextCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c777de-8893-4585-b06e-bb7f61fec241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I may need to make this function be able to batch input together (or I can later create a collate function)\n",
    "#pad token = '$'\n",
    "def tokenize(sents, global_phonemizer, tokenizer, text_cleaner):\n",
    "    batched = []\n",
    "    max_id_length = 0\n",
    "    for sent in sents:\n",
    "        pretextcleaned = ' '.join(phonemize(sent, global_phonemizer, tokenizer)['phonemes'])\n",
    "        cleaned = text_cleaner(pretextcleaned)\n",
    "        batched.append(torch.LongTensor(cleaned))\n",
    "        max_id_length = max(max_id_length, len(cleaned))\n",
    "    phoneme_ids = torch.zeros((len(sents), max_id_length)).long()\n",
    "    mask = torch.zeros((len(sents), max_id_length)).long()\n",
    "    for i, c in enumerate(batched):\n",
    "        phoneme_ids[i,:len(c)] = c\n",
    "        mask[i,:len(c)] = 1\n",
    "    return phoneme_ids, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3607656b-1e76-478f-98bc-767110b28260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pltbert_embs(s, global_phonemizer, tokenizer, text_cleaner):\n",
    "    \"\"\"\n",
    "    Input: list of texts\n",
    "\n",
    "    Output: output of pretrained Albert model - (batch_size, num_tokens, 768)\n",
    "    \"\"\"\n",
    "    phoneme_ids, attention_mask = tokenize(s, global_phonemizer, tokenizer, text_cleaner)\n",
    "    return plbert(phoneme_ids, attention_mask=attention_mask).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89cba010-ef1f-4d26-b92a-06cc5fcb6fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 123, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pltbert_embs(batch_of_text, global_phonemizer, tokenizer, text_cleaner).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00f9de-2395-43bb-87ed-40c99d23f816",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SpeakerNet Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62760cb-310d-473f-8551-f87c677fcd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spNet = torch.jit.load(\"/Users/ajaybati/Downloads/0.23.0612.1.AT.PHL.alorica/spNet_traced.jit\")\n",
    "ecapa = torch.jit.load(\"ecapa2_traced.jit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5dea42f-ed55-4d90-8648-8fe6f7604e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 480000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios = [\"miipherTestDataset/train/clean_trainset_wav/p234_001.wav\", \"miipherTestDataset/train/clean_trainset_wav/p234_002.wav\",\n",
    "         \"miipherTestDataset/train/clean_trainset_wav/p234_003.wav\", \"miipherTestDataset/train/clean_trainset_wav/p234_004.wav\"]\n",
    "audios = audios*2\n",
    "audio_loaded = []\n",
    "for x in audios:\n",
    "    ref_path = x\n",
    "    audio, sr = librosa.load(ref_path, sr=16000)\n",
    "    audio = np.array([audio])\n",
    "    audio_signal = torch.tensor(whisper.pad_or_trim(audio))\n",
    "    audio_loaded.append(audio_signal)\n",
    "audio_loaded = torch.stack(audio_loaded).squeeze()\n",
    "audio_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d173af31-5e25-41d8-ace0-374ba30c18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_spenc(batch):\n",
    "    kwarg = {'input_signal': batch.cuda(), 'input_signal_length': torch.tensor([batch.shape[-1]]*len(batch)).cuda()} #batched input\n",
    "    return kwarg\n",
    "kwarg = batch_spenc(audio_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8f486b5-0d9b-41f8-8713-c6460d1f3419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 192])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecapa(**kwarg)[-1].shape #logits, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522dbdcb-e2d9-48e7-b5e1-297803a6b6ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Speed Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23684f9b-214d-4f26-a18e-c79294506594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def create_input(seconds):\n",
    "    return torch.randn(1,16000*seconds)\n",
    "\n",
    "ten = create_input(10)\n",
    "thirty = create_input(30)\n",
    "twomin = create_input(120)\n",
    "fivemin = create_input(300)\n",
    "\n",
    "import time\n",
    "def test(input):\n",
    "    kwarg = {'input_signal': input, 'input_signal_length': torch.tensor([input.shape[-1]])}\n",
    "    spNet_times = []\n",
    "    for x in range(20):\n",
    "        start = time.time()\n",
    "        out = spNet(**kwarg)\n",
    "        spNet_times.append(time.time()-start)\n",
    "\n",
    "    ecapa_times = []\n",
    "    for x in range(20):\n",
    "        start = time.time()\n",
    "        out = ecapa(**kwarg)\n",
    "        ecapa_times.append(time.time()-start)\n",
    "\n",
    "    return spNet_times, ecapa_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79752847-e403-4e89-82c6-34f4fb2ec447",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adffb502-a30d-4161-ad5a-d16292bc9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26a5afeb-ab92-4629-ad39-3b8e34109dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sp, ec = test(fivemin)\n",
    "\n",
    "sp = np.array(sp)\n",
    "ec = np.array(ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41e13e45-3329-4c83-ac42-f0b98dd3b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "times.append([(np.median(ec), ec.mean()), (np.median(sp), sp.mean())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea5138a6-d504-44f9-8fc5-608b2ee7cba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.0795280933380127, 0.07995404005050659),\n",
       "  (0.03753793239593506, 0.041062581539154056)],\n",
       " [(0.2759588956832886, 0.27689037322998045),\n",
       "  (0.09117209911346436, 0.09421477317810059)],\n",
       " [(1.3495440483093262, 1.353237557411194),\n",
       "  (0.4264800548553467, 0.42521281242370607)],\n",
       " [(4.634666442871094, 4.688384139537812),\n",
       "  (0.9876257181167603, 0.9849279642105102)]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f90d39-6055-4804-a085-2b9a3170643f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Whisper Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ac29a10-9b49-42ec-bdb0-e9f4f6c246f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # PyTorch thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd9a9652-1a33-4a4e-9b11-f33e54ee4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85405bec-4de1-40d8-a009-2fefbfbc4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"small.en\")\n",
    "\n",
    "# # load audio and pad/trim it to fit 30 seconds\n",
    "# audio = whisper.load_audio(\"/Users/ajaybati/Downloads/0.23.0612.1.AT.PHL.alorica/out.wav\")\n",
    "# audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# # make log-Mel spectrogram and move to the same device as the model\n",
    "# mel = whisper.log_mel_spectrogram(audio)\n",
    "\n",
    "# model.embed_audio(mel.reshape(1,*mel.shape)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96ec768c-b236-465a-bee0-25b198feb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper_encode_batch import Batch, process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1352c1c4-ca27-427c-a1b5-b056f700d7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23075"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios = [\"miipherTestDataset/train/clean_trainset_wav/\"+x for x in sorted(os.listdir(\"miipherTestDataset/train/clean_trainset_wav/\"))]\n",
    "len(audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19ba11cd-00f5-410e-af66-d2492642c38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 23075/23075 [1:10:03<00:00,  5.49it/s]\n"
     ]
    }
   ],
   "source": [
    "process_data(audios, Batch(3,400), \"whisperEmbs/clean_trainset_embs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12254155-df17-4fc7-b7f7-ad23a95ab4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23075"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios = [\"miipherTestDataset/train/noisy_trainset_wav/\"+x for x in sorted(os.listdir(\"miipherTestDataset/train/noisy_trainset_wav/\"))]\n",
    "len(audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78dd3a68-f996-4782-a7eb-ad52ba3ec293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 23075/23075 [1:09:35<00:00,  5.53it/s]\n"
     ]
    }
   ],
   "source": [
    "process_data(audios, Batch(3,400), \"whisperEmbs/noisy_trainset_embs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ccbb9ef-252e-4f3d-8952-7a0b5cf0452e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "824"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios = [\"miipherTestDataset/test/clean_testset_wav/\"+x for x in sorted(os.listdir(\"miipherTestDataset/test/clean_testset_wav/\"))]\n",
    "len(audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd074b54-f16d-435b-872c-5c8affb796b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 824/824 [01:41<00:00,  8.08it/s]\n"
     ]
    }
   ],
   "source": [
    "process_data(audios, Batch(3,400), \"whisperEmbs/clean_testset_embs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d4af309-8cbc-4d06-90bc-b9b73b181e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "824"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios = [\"miipherTestDataset/test/noisy_testset_wav/\"+x for x in sorted(os.listdir(\"miipherTestDataset/test/noisy_testset_wav/\"))]\n",
    "len(audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e85f2311-bb9a-4248-af0d-84084615405a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 824/824 [01:45<00:00,  7.79it/s]\n"
     ]
    }
   ],
   "source": [
    "process_data(audios, Batch(3,400), \"whisperEmbs/noisy_testset_embs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60ee2eb1-347d-4ba8-b101-3bea574d2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = np.load(\"whisperEmbs/noisy_testset_embs/batch400_2.npy\", mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90c03d7b-9ff6-4ba6-b0fd-bbc2167f000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61b165-54f7-4b1e-b92e-cd871df77001",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, _ = librosa.load(\"miipherTestDataset/test/clean_testset_wav/p257_433.wav\", sr=16000, duration=20)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio)\n",
    "\n",
    "test = model.embed_audio(mel.reshape(1,*mel.shape).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a83ffc-8f3b-4bb7-b901-202df91e8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "query[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eda7076-67a5-4ea0-890c-70fefe0915f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2969, -0.1863,  0.2197, -0.3074, -0.9798, -0.3804, -1.6542,  0.2116,\n",
       "        -1.4315,  0.3202], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.squeeze()[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "395534bf-d7d5-40b1-a419-83597b601149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'miipherTestDataset/test/noisy_testset_wav/p257_433.wav'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios = [\"miipherTestDataset/test/noisy_testset_wav/\"+x for x in sorted(os.listdir(\"miipherTestDataset/test/noisy_testset_wav/\"))]\n",
    "audios[822]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5989bd59-c381-42d6-aac6-d99fa7879b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisperEmbTrainClean = 'whisperEmbs/clean_trainset_embs'\n",
    "whisperEmbTrainNoisy = 'whisperEmbs/noisy_trainset_embs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83f649cf-8f74-4938-8d2f-468cc5a01108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "whisperembclean = [f\"{whisperEmbTrainClean}/{a}\" for a in sorted([f for f in os.listdir(whisperEmbTrainClean) if 'batch' in f], key=lambda x: int(x.split('_')[-1].split('.')[0]))]\n",
    "whisperembnoisy = [f\"{whisperEmbTrainNoisy}/{a}\" for a in sorted([f for f in os.listdir(whisperEmbTrainNoisy) if 'batch' in f], key=lambda x: int(x.split('_')[-1].split('.')[0]))]\n",
    "clean = sorted(os.listdir(\"miipherTestDataset/train/clean_trainset_wav/\"))\n",
    "noisy = sorted(os.listdir(\"miipherTestDataset/train/noisy_trainset_wav/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "835f76ff-44b4-45fe-9048-133caf09afb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whisperEmbs/clean_trainset_embs/batch400_1.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_2.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_3.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_4.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_5.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_6.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_7.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_8.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_9.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_10.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_11.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_12.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_13.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_14.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_15.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_16.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_17.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_18.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_19.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_20.npy',\n",
       " 'whisperEmbs/clean_trainset_embs/batch400_21.npy']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisperembclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92c708-18b1-4753-8e98-687aba5d7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = [\"miipherTestDataset/test/noisy_testset_wav/\"+x for x in sorted(os.listdir(\"miipherTestDataset/test/noisy_testset_wav/\"))]\n",
    "\n",
    "audios2 = [\"miipherTestDataset/test/clean_testset_wav/\"+x for x in sorted(os.listdir(\"miipherTestDataset/test/clean_testset_wav/\"))]\n",
    "[(a,b) for a,b in zip(audios,audios2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca1e91-b6c0-468e-a572-bbff4a4aa794",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f085a34-4da4-473e-beda-1c93b2e1609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c8915dc-65fd-4e63-a523-8e4d1bda8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "class MiipherDataset(Dataset):\n",
    "    def __init__(self, noisy_filepath, clean_filepath, text_filepath, whisperEmbfilepathNoisy, whisperEmbfilepathClean):\n",
    "        self.noisyfilepaths = noisy_filepath\n",
    "        self.cleanfilepaths = clean_filepath\n",
    "        self.sentpaths = text_filepath\n",
    "        self.spencembs = []\n",
    "        self.all_sents = []\n",
    "        \n",
    "        #change to enable batched input\n",
    "        for x in tqdm(range(len(self.noisyfilepaths))):\n",
    "            noisy, clean, text = self.noisyfilepaths[x], self.cleanfilepaths[x], self.sentpaths[x]\n",
    "\n",
    "            #Text: load the text from the file as a string\n",
    "            with open(text, 'r') as f:\n",
    "                self.all_sents.append(f.read().strip())\n",
    "                \n",
    "            #Speaker Encoder: load audio file and save the loaded array\n",
    "            ref_path = noisy\n",
    "            audio, sr = librosa.load(ref_path, sr=16000)\n",
    "            audio = np.array([audio])\n",
    "            audio_signal = torch.tensor(whisper.pad_or_trim(audio))\n",
    "            self.spencembs.append(audio_signal)\n",
    "\n",
    "        batchNoises = []\n",
    "        for batchNoisy in tqdm(whisperEmbfilepathNoisy):\n",
    "            batchNoises.append(np.load(batchNoisy, mmap_mode='r'))\n",
    "        num_batches = sum([a.shape[0] for a in batchNoises])\n",
    "        self.whisper_noisy = []\n",
    "        for loaded in batchNoises:\n",
    "            for i in tqdm(range(len(loaded))):\n",
    "                self.whisper_noisy.append(loaded[i,:,:])\n",
    "        \n",
    "        batchCleans = []\n",
    "        for batchClean in tqdm(whisperEmbfilepathClean):\n",
    "            batchCleans.append(np.load(batchClean, mmap_mode='r'))\n",
    "        num_batches = sum([a.shape[0] for a in batchNoises])\n",
    "        self.whisper_clean = []\n",
    "        for loaded in batchCleans:\n",
    "            for i in tqdm(range(len(loaded))):\n",
    "                self.whisper_clean.append(loaded[i,:,:])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.noisyfilepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.whisper_noisy[idx], self.spencembs[idx], self.all_sents[idx], self.whisper_clean[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61656e17-7d61-4995-8821-c7b79a442e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch = list of tuples(whisper_noisy, speaker wav, raw sentences, whisper_clean)\n",
    "    \"\"\"\n",
    "    whisper_noisy, speaker_wav, raw_sents, whisper_clean = [],[],[],[]\n",
    "    for a,b,c,d in batch:\n",
    "        whisper_noisy.append(a)\n",
    "        speaker_wav.append(b)\n",
    "        raw_sents.append(c) \n",
    "        whisper_clean.append(d)\n",
    "    whisper_noisy = torch.stack(whisper_noisy)\n",
    "    whisper_clean = torch.stack(whisper_clean)\n",
    "    print(\"Here\")\n",
    "    plbertembs = get_pltbert_embs(raw_sents, global_phonemizer, tokenizer, text_cleaner)\n",
    "\n",
    "    print(\"Done with plbert, now speaker Emb\")\n",
    "    speaker_wav = torch.stack(speaker_wav).squeeze()\n",
    "    batched = batch_spenc(speaker_wav)\n",
    "    speakerembs = ecapa(**batched)[-1]\n",
    "    print(\"speaker embs done\")\n",
    "    return plbertembs, whisper_noisy, speakerembs, whisper_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f0b431-efa8-427e-b4e5-ad1aa9e744c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import RandomSampler\n",
    "\n",
    "# BATCH_SIZE = 8\n",
    "# # train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# train_iterator = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn) #, sampler=train_sampler\n",
    "\n",
    "# whisperembclean = [f\"whisperEmbs/{a}\" for a in os.listdir(\"whisperEmbs/\")[:-1]]\n",
    "# whisperembnoisy = [f\"whisperEmbs/{a}\" for a in os.listdir(\"whisperEmbs/\")[:-1]]\n",
    "# clean = [f\"miipherTestDataset/clean_testset_wav/{a}\" for a in sorted(os.listdir(\"miipherTestDataset/clean_testset_wav/\"))]\n",
    "# noisy = [f\"miipherTestDataset/noisy_testset_wav/{a}\" for a in sorted(os.listdir(\"miipherTestDataset/noisy_testset_wav/\"))]\n",
    "# text = [f\"miipherTestDataset/testset_txt/{a}\" for a in sorted(os.listdir(\"miipherTestDataset/testset_txt\"))]\n",
    "\n",
    "# train_data = MiipherDataset(noisy, clean, text, whisperembnoisy, whisperembclean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12bb27b3-6536-40eb-9f9f-ff39abd92db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# test_wav_clean = 'miipherTestDataset/test/clean_testset_wav'\n",
    "# test_wav_noisy = 'miipherTestDataset/test/noisy_testset_wav'\n",
    "# test_txt = 'miipherTestDataset/test/testset_txt'\n",
    "# clean = [f\"{test_wav_clean}/{a}\" for a in sorted(os.listdir(test_wav_clean))]\n",
    "# noisy = [f\"{test_wav_noisy}/{a}\" for a in sorted(os.listdir(test_wav_noisy))]\n",
    "# text = [f\"{test_txt}/{a}\" for a in sorted(os.listdir(test_txt))]\n",
    "# d = [(a,b,c) for a,b,c in zip(clean,noisy,text) if 'wav' in a+b+c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e67c6a0-60d0-4738-bcec-7dc0a367a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "class MiipherLightningModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, collate_fn):\n",
    "        super().__init__()\n",
    "        self.collate_fn = collate_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.whisperEmbTrainClean = 'whisperEmbs/clean_trainset_embs'\n",
    "        self.whisperEmbTrainNoisy = 'whisperEmbs/noisy_trainset_embs'\n",
    "        self.whisperEmbTestClean = 'whisperEmbs/clean_testset_embs'\n",
    "        self.whisperEmbTestNoisy = 'whisperEmbs/noisy_testset_embs'\n",
    "        self.train_wav_clean = 'miipherTestDataset/train/clean_trainset_wav'\n",
    "        self.train_wav_noisy = 'miipherTestDataset/train/noisy_trainset_wav'\n",
    "        self.train_txt = 'miipherTestDataset/train/trainset_txt'\n",
    "        self.test_wav_clean = 'miipherTestDataset/test/clean_testset_wav'\n",
    "        self.test_wav_noisy = 'miipherTestDataset/test/noisy_testset_wav'\n",
    "        self.test_txt = 'miipherTestDataset/test/testset_txt'\n",
    "    \n",
    "    def setup(self, stage):\n",
    "        #TRAIN\n",
    "        # whisperembclean = [f\"{self.whisperEmbTrainClean}/{a}\" for a in sorted([f for f in os.listdir(self.whisperEmbTrainClean) if 'batch' in f], key=lambda x: int(x.split('_')[-1].split('.')[0]))]\n",
    "        # whisperembnoisy = [f\"{self.whisperEmbTrainNoisy}/{a}\" for a in sorted([f for f in os.listdir(self.whisperEmbTrainNoisy) if 'batch' in f], key=lambda x: int(x.split('_')[-1].split('.')[0]))]\n",
    "        # clean = [f\"{self.train_wav_clean}/{a}\" for a in sorted(os.listdir(self.train_wav_clean))]\n",
    "        # noisy = [f\"{self.train_wav_noisy}/{a}\" for a in sorted(os.listdir(self.train_wav_noisy))]\n",
    "        # text = [f\"{self.train_txt}/{a}\" for a in sorted(os.listdir(self.train_txt))]\n",
    "        # self.miipher_train = MiipherDataset(noisy, clean, text, whisperembnoisy, whisperembclean)\n",
    "\n",
    "        #TEST\n",
    "        whisperembclean = [f\"{self.whisperEmbTestClean}/{a}\" for a in sorted([f for f in os.listdir(self.whisperEmbTestClean) if 'batch' in f], key=lambda x: int(x.split('_')[-1].split('.')[0]))]\n",
    "        whisperembnoisy = [f\"{self.whisperEmbTestNoisy}/{a}\" for a in sorted([f for f in os.listdir(self.whisperEmbTestNoisy) if 'batch' in f], key=lambda x: int(x.split('_')[-1].split('.')[0]))]\n",
    "        clean = [f\"{self.test_wav_clean}/{a}\" for a in sorted(os.listdir(self.test_wav_clean))]\n",
    "        noisy = [f\"{self.test_wav_noisy}/{a}\" for a in sorted(os.listdir(self.test_wav_noisy))]\n",
    "        text = [f\"{self.test_txt}/{a}\" for a in sorted(os.listdir(self.test_txt))]\n",
    "        self.miipher_test = MiipherDataset(noisy, clean, text, whisperembnoisy, whisperembclean)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.miipher_train, batch_size=self.batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.miipher_test, batch_size=self.batch_size, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d934a5fd-bfbd-4c61-b694-911159ca04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = MiipherLightningModule(4, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "192ab130-6e27-457c-b4da-19513d7e8d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b820661fc980483ba30b4e30e2a5879c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d463710aeb3443f695717e1bf40e9f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c96a0a02a84904baa8c96f96b019db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2c57fec3b24f4695d1d39207126f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585631baf60e4dc5bb44059d57322498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f778069445fb405ab100e29ee2e05450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcccfceb4134433099f95bd57c1a56d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d.setup(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eccfbd5a-7218-43d1-b4ad-a036c2845a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([[-0.80728567, -0.41651925,  0.22006467, ...,  1.21752286,\n",
       "           0.61488628,  1.4338907 ],\n",
       "         [ 0.09944215,  0.35345295,  0.40228635, ...,  0.79812413,\n",
       "          -0.41561455,  0.63997132],\n",
       "         [ 0.64994407,  0.40558368,  0.09346313, ..., -1.13413262,\n",
       "           1.2571727 , -0.55739397],\n",
       "         ...,\n",
       "         [-0.05057729, -0.03693019, -0.03641871, ..., -0.0162362 ,\n",
       "           0.00329678, -0.00825675],\n",
       "         [-1.67652476, -0.28942066,  0.3632668 , ...,  0.26832759,\n",
       "          -1.15833521,  1.09190559],\n",
       "         [-0.87837517, -0.14397915,  0.12409495, ...,  0.74352312,\n",
       "          -0.74356645,  0.60087365]]),\n",
       " tensor([[-0.0113, -0.0181, -0.0153,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 'Ask her to bring these things with her from the store.',\n",
       " memmap([[-0.42953297, -0.6658935 ,  0.09071337, ...,  1.18186104,\n",
       "          -0.14253475,  1.30860972],\n",
       "         [ 0.53896588, -0.06958443, -0.15426952, ..., -0.78864527,\n",
       "           0.20755263, -0.21173766],\n",
       "         [ 1.41217983,  0.60096401, -0.07772667, ..., -0.89371037,\n",
       "          -0.03627483, -0.4462865 ],\n",
       "         ...,\n",
       "         [-0.04709931, -0.03714079, -0.03647976, ..., -0.01558192,\n",
       "           0.00418532, -0.00962696],\n",
       "         [-1.69823134, -0.29493713,  0.39156643, ...,  0.26639   ,\n",
       "          -1.22567761,  1.02577972],\n",
       "         [-0.87179363, -0.20579045,  0.23776527, ...,  0.69266039,\n",
       "          -0.90473193,  0.49989125]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.miipher_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92412b04-e964-4ecf-9041-4dd98a99b898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2210, -0.5719, -0.3844,  ...,  1.0616, -0.4346,  1.1844],\n",
       "        [ 0.8678,  0.6076,  0.0555,  ..., -0.0454, -0.4874,  0.8251],\n",
       "        [ 0.7029, -0.2993,  0.0506,  ...,  0.5186, -0.7027,  0.4898],\n",
       "        ...,\n",
       "        [-0.0417, -0.0383, -0.0326,  ..., -0.0070,  0.0041, -0.0073],\n",
       "        [-1.6980, -0.2917,  0.4673,  ...,  0.4527, -1.3656,  1.0160],\n",
       "        [-0.8866, -0.4458,  0.4003,  ...,  0.6203, -0.9785,  0.3831]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(d.miipher_test.whisper_clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "c0cf199e-0a36-4305-b5db-1bb21c3a3b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "Done with plbert, now speaker Emb\n",
      "speaker embs done\n",
      "torch.Size([8, 116, 768]) torch.Size([8, 1500, 768]) torch.Size([8, 192]) torch.Size([8, 1500, 768])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for a,b,c,d in train_iterator:\n",
    "    print(a.shape,b.shape,c.shape,d.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ef5f5e2-f324-4de9-9574-a4bbdfcd83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "loss = nn.L1Loss()\n",
    "loss2 = nn.MSELoss()\n",
    "input = torch.randn(8,1500, 768)\n",
    "target = torch.randn(8,1500, 768)\n",
    "zeros = torch.zeros_like(target)\n",
    "output = loss(input, target)\n",
    "output2 = loss2(input,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e73b308-0c5a-4cb9-ae83-03b7c1034adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9999)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55978ecf-490a-454c-97ab-64f0cd2db126",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = abs(target - input)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e912fdd7-6cdb-495b-989e-db9805159d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9999)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a**2).sum()/(8*1500*768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2ebdc5-2ee7-46c6-9300-b1c1dc9e6c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9999)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2(target,zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd635365-e4d3-4f50-a42d-8e3119107083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9999)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target**2).sum()/np.prod(list(target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c998950f-5a82-436f-8576-73ca670ea228",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7280b885-d04f-46d2-928c-ba6940e44067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiipherDataset(Dataset):\n",
    "    def __init__(self, noisy_filepath, clean_filepath, text_filepath, whisperEmbfilepathNoisy, whisperEmbfilepathClean):\n",
    "        self.noisyfilepaths = noisy_filepath\n",
    "        self.cleanfilepaths = clean_filepath\n",
    "        self.sentpaths = text_filepath\n",
    "        self.spencembs = []\n",
    "        self.all_sents = []\n",
    "        \n",
    "        #change to enable batched input\n",
    "        for x in tqdm(range(len(self.noisyfilepaths))):\n",
    "            noisy, clean, text = self.noisyfilepaths[x], self.cleanfilepaths[x], self.sentpaths[x]\n",
    "\n",
    "            #Text: load the text from the file as a string\n",
    "            with open(text, 'r') as f:\n",
    "                self.all_sents.append(f.read().strip())\n",
    "                \n",
    "            #Speaker Encoder: load audio file and save the loaded array\n",
    "            # ref_path = noisy\n",
    "            # audio, sr = librosa.load(ref_path, sr=16000)\n",
    "            # audio = np.array([audio])\n",
    "            # audio_signal = torch.tensor(whisper.pad_or_trim(audio))\n",
    "            self.spencembs.append(noisy)\n",
    "\n",
    "        batchNoises = []\n",
    "        for batchNoisy in tqdm(whisperEmbfilepathNoisy):\n",
    "            batchNoises.append(np.load(batchNoisy, mmap_mode='r'))\n",
    "        num_batches = sum([a.shape[0] for a in batchNoises])\n",
    "        self.whisper_noisy = []\n",
    "        for loaded in batchNoises:\n",
    "            for i in tqdm(range(len(loaded))):\n",
    "                self.whisper_noisy.append(loaded[i,:,:])\n",
    "        \n",
    "        batchCleans = []\n",
    "        for batchClean in tqdm(whisperEmbfilepathClean):\n",
    "            batchCleans.append(np.load(batchClean, mmap_mode='r'))\n",
    "        num_batches = sum([a.shape[0] for a in batchNoises])\n",
    "        self.whisper_clean = []\n",
    "        for loaded in batchCleans:\n",
    "            for i in tqdm(range(len(loaded))):\n",
    "                self.whisper_clean.append(loaded[i,:,:])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.noisyfilepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.whisper_noisy[idx], self.spencembs[idx], self.all_sents[idx], self.whisper_clean[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch = list of tuples(whisper_noisy, speaker wav, raw sentences, whisper_clean)\n",
    "    \"\"\"\n",
    "    whisper_noisy, speaker_wav, raw_sents, whisper_clean = [],[],[],[]\n",
    "    for a,b,c,d in batch:\n",
    "        whisper_noisy.append(torch.tensor(a, dtype=torch.float32))\n",
    "        \n",
    "        ref_path = b\n",
    "        audio, sr = librosa.load(ref_path, sr=16000)\n",
    "        audio = np.array([audio])\n",
    "        audio_signal = torch.tensor(whisper.pad_or_trim(audio))\n",
    "        speaker_wav.append(audio_signal)\n",
    "        \n",
    "        raw_sents.append(c) \n",
    "        whisper_clean.append(torch.tensor(d,dtype=torch.float32))\n",
    "    whisper_noisy = torch.stack(whisper_noisy)\n",
    "    whisper_clean = torch.stack(whisper_clean)\n",
    "\n",
    "    plbertembs = get_pltbert_embs(raw_sents, global_phonemizer, tokenizer, text_cleaner)\n",
    "\n",
    "    \n",
    "    speaker_wav = torch.stack(speaker_wav).squeeze()\n",
    "    batched = batch_spenc(speaker_wav)\n",
    "    speakerembs = ecapa(**batched)[-1]\n",
    "\n",
    "    return plbertembs.cpu().detach(), whisper_noisy.cpu().detach(), speakerembs.cpu().detach(), whisper_clean.cpu().detach(), raw_sents\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Lightning Module \n",
    "\"\"\"\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "class MiipherLightningModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, collate_fn):\n",
    "        super().__init__()\n",
    "        self.collate_fn = collate_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.whisperEmbTrainClean = 'whisperEmbs/clean_trainset_embs'\n",
    "        self.whisperEmbTrainNoisy = 'whisperEmbs/noisy_trainset_embs'\n",
    "        self.whisperEmbTestClean = 'whisperEmbs/clean_testset_embs'\n",
    "        self.whisperEmbTestNoisy = 'whisperEmbs/noisy_testset_embs'\n",
    "        self.train_wav_clean = 'miipherTestDataset/train/clean_trainset_wav'\n",
    "        self.train_wav_noisy = 'miipherTestDataset/train/noisy_trainset_wav'\n",
    "        self.train_txt = 'miipherTestDataset/train/trainset_txt'\n",
    "        self.test_wav_clean = 'miipherTestDataset/test/clean_testset_wav'\n",
    "        self.test_wav_noisy = 'miipherTestDataset/test/noisy_testset_wav'\n",
    "        self.test_txt = 'miipherTestDataset/test/testset_txt'\n",
    "    \n",
    "    def setup(self, stage):\n",
    "        #TRAIN\n",
    "        whisperembclean = [f\"{self.whisperEmbTrainClean}/{a}\" for a in sorted([f for f in os.listdir(self.whisperEmbTrainClean) if 'batch' in f], key=lambda x: int(x.split('_')[-1].split('.')[0]))]\n",
    "        whisperembnoisy = [f\"{self.whisperEmbTrainNoisy}/{a}\" for a in sorted([f for f in os.listdir(self.whisperEmbTrainNoisy) if 'batch' in f], key=lambda x: int(x.split('_')[-1].split('.')[0]))]\n",
    "        clean = [f\"{self.train_wav_clean}/{a}\" for a in sorted(os.listdir(self.train_wav_clean))]\n",
    "        noisy = [f\"{self.train_wav_noisy}/{a}\" for a in sorted(os.listdir(self.train_wav_noisy))]\n",
    "        text = [f\"{self.train_txt}/{a}\" for a in sorted(os.listdir(self.train_txt))]\n",
    "        self.miipher_train = MiipherDataset(noisy, clean, text, whisperembnoisy, whisperembclean)\n",
    "\n",
    "        #TEST\n",
    "        whisperembclean = [f\"{self.whisperEmbTestClean}/{a}\" for a in sorted([f for f in os.listdir(self.whisperEmbTestClean) if 'batch' in f], key=lambda x: int(x.split('_')[-1].split('.')[0]))]\n",
    "        whisperembnoisy = [f\"{self.whisperEmbTestNoisy}/{a}\" for a in sorted([f for f in os.listdir(self.whisperEmbTestNoisy) if 'batch' in f], key=lambda x: int(x.split('_')[-1].split('.')[0]))]\n",
    "        clean = [f\"{self.test_wav_clean}/{a}\" for a in sorted(os.listdir(self.test_wav_clean))]\n",
    "        noisy = [f\"{self.test_wav_noisy}/{a}\" for a in sorted(os.listdir(self.test_wav_noisy))]\n",
    "        text = [f\"{self.test_txt}/{a}\" for a in sorted(os.listdir(self.test_txt))]\n",
    "        self.miipher_test = MiipherDataset(noisy, clean, text, whisperembnoisy, whisperembclean)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        # print(\"-\"*10 + \"Data Loading Sanity Check\" + \"-\"*10)\n",
    "        # _, a, b, _ = self.miipher_train[5]\n",
    "        # print(a,b)\n",
    "        # print(\"-\"*10 + \"Data Loading Sanity Check DONE\" + \"-\"*10)\n",
    "        return DataLoader(self.miipher_train, batch_size=self.batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # print(\"-\"*10 + \"Data Loading Sanity Check\" + \"-\"*10)\n",
    "        # _, a, b, _ = self.miipher_test[5]\n",
    "        # print(a,b)\n",
    "        # print(\"-\"*10 + \"Data Loading Sanity Check DONE\" + \"-\"*10)\n",
    "        return DataLoader(self.miipher_test, batch_size=self.batch_size, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71287d9e-5919-43b5-bfe8-1ba2561e6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTest = MiipherLightningModule(3, collate_fn)\n",
    "dataTest.setup(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ebc0a7-a291-4f88-a4c3-3999449f3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = dataTest.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5056ab34-1722-4bcc-8e50-01ca0bef68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36011d7f-24ed-4d6c-951f-432bbef5afcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-07-27 19:31:18 nemo_logging:349] /opt/conda/envs/sanas2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501: UserWarning: FALLBACK path has been taken inside: runCudaFusionGroup. This is an indication that codegen Failed for some reason.\n",
      "    To debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`\n",
      "     (Triggered internally at ../third_party/nvfuser/csrc/manager.cpp:335.)\n",
      "      return forward_call(*args, **kwargs)\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.',\n",
       " 'When the sunlight strikes raindrops in the air, they act as a prism and form a rainbow.',\n",
       " 'The rainbow is a division of white light into many beautiful colors.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iterator)\n",
    "batch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca2b980e-bd55-4ef7-ba85-ff8177087e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.',\n",
       " 'When the sunlight strikes raindrops in the air, they act as a prism and form a rainbow.',\n",
       " 'The rainbow is a division of white light into many beautiful colors.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plbert_embs, whisper_noisy, ecapa_embs, whisper_clean, raw_sents = batch\n",
    "raw_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be4e4638-257e-4dfe-8441-4fb37a8d8de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0519,  0.0268,  0.2603,  ...,  1.1658, -0.5511,  0.8337],\n",
       "        [ 0.2664,  0.6931,  0.0195,  ...,  0.6229, -0.9968,  1.4332],\n",
       "        [ 0.7123,  0.7288, -0.2482,  ..., -0.0896, -0.8955,  0.8289],\n",
       "        ...,\n",
       "        [-0.0462, -0.0370, -0.0352,  ..., -0.0165,  0.0076, -0.0127],\n",
       "        [-1.6867, -0.2436,  0.3077,  ...,  0.2227, -1.0488,  1.1712],\n",
       "        [-0.8994, -0.0606,  0.0365,  ...,  0.5178, -0.5199,  0.7744]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_noisy[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d8dd3dd-c158-441d-ba9b-2e8d67e6e5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3804, -0.2556, -0.0031,  ...,  0.8562, -0.3259,  1.2493],\n",
       "        [ 0.4227,  0.6743, -0.1595,  ...,  0.0965, -0.5177,  0.8075],\n",
       "        [ 0.5985,  0.7990, -0.2802,  ..., -0.0729, -0.3918,  0.4857],\n",
       "        ...,\n",
       "        [-0.0409, -0.0365, -0.0354,  ..., -0.0131,  0.0075, -0.0129],\n",
       "        [-1.7670, -0.2528,  0.4785,  ...,  0.3382, -1.3123,  1.0631],\n",
       "        [-0.9448, -0.2337,  0.3379,  ...,  0.6196, -0.8520,  0.6278]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_clean[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "515b8c53-0d45-4279-a923-62953eea3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"small.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e183b40c-e4be-45a4-80aa-853538396bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(path):\n",
    "    wav, _ = librosa.load(path, sr=16000, duration=20)\n",
    "    \n",
    "    # write mel and metadata to batch\n",
    "    wav = whisper.pad_or_trim(wav)\n",
    "    mel = whisper.log_mel_spectrogram(wav)\n",
    "    return model.embed_audio(mel.reshape(1,*mel.shape).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74adc357-6f81-49b1-b876-a40931c085db",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = convert(\"miipherTestDataset/test/clean_testset_wav/p232_007.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2880ca03-2c39-403e-9d1e-2977b4614ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(a.detach().cpu().squeeze(),whisper_noisy[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
