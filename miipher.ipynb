{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5132c9ba-9919-4901-8ee1-1c1a2eda5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models import EncDecSpeakerLabelModel\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as F\n",
    "import math\n",
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "import wave\n",
    "import sys\n",
    "import contextlib\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import yaml\n",
    "from transformers import AlbertConfig, AlbertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6231d066-0011-421a-8a8a-3fa5f85482b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb97d8-7c45-4eff-8d1a-c5754921573e",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eca8b3-bd9c-4a7f-8c9f-691bae38680f",
   "metadata": {},
   "source": [
    "## [PL-BERT](https://github.com/yl4579/PL-BERT) Pretrained (1M steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9027a6f3-e62d-48bc-9ea7-b7bb0250edf1",
   "metadata": {},
   "source": [
    "### Loading PL-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac32468c-c840-4d02-9f8c-b790b759eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plbert_root = \"plbert/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf242bae-411d-4007-9c53-9a607a0cc7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plbert/Checkpoint/step_1000000.t7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = plbert_root+\"Checkpoint/\"\n",
    "config_path = os.path.join(log_dir, \"config.yml\")\n",
    "plbert_config = yaml.safe_load(open(config_path))\n",
    "\n",
    "albert_base_configuration = AlbertConfig(**plbert_config['model_params'])\n",
    "plbert = AlbertModel(albert_base_configuration)\n",
    "\n",
    "files = os.listdir(log_dir)\n",
    "ckpts = []\n",
    "for f in os.listdir(log_dir):\n",
    "    if f.startswith(\"step_\"): ckpts.append(f)\n",
    "\n",
    "iters = [int(f.split('_')[-1].split('.')[0]) for f in ckpts if os.path.isfile(os.path.join(log_dir, f))]\n",
    "iters = sorted(iters)[-1]\n",
    "\n",
    "checkpoint = torch.load(log_dir + \"step_\" + str(iters) + \".t7\", map_location='cpu')\n",
    "print(log_dir + \"step_\" + str(iters) + \".t7\")\n",
    "state_dict = checkpoint['net']\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    if name.startswith('encoder.'):\n",
    "        name = name[8:] # remove `encoder.`\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "plbert.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c5015-e0b4-4b7e-b6dd-9547df5c27c3",
   "metadata": {},
   "source": [
    "### Loading tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8257cc97-e578-47a1-bcd3-9c1e6b1d0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"plbert/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20d8c0f8-bdec-4cc0-a104-9381e973dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plbert.phonemize import phonemize\n",
    "from phonemizer.backend import EspeakBackend\n",
    "from transformers import TransfoXLTokenizer\n",
    "from plbert.text_utils import TextCleaner\n",
    "from plbert.text_normalize import normalize_text, remove_accents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528f2e04-863c-490a-980b-90f4b0fce489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "global_phonemizer = EspeakBackend(language='en-us', preserve_punctuation=True, with_stress=True) #make sure brew install espeak and export location of .dylib\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(plbert_config['dataset_params']['tokenizer'])\n",
    "text = \"And also can you please check what is the current temperature setting of your unit both fridge and the freezer?\"\n",
    "text_cleaner = TextCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a738c866-a78c-47fd-9294-ee27557480c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I may need to make this function be able to batch input together (or I can later create a collate function)\n",
    "#pad token = '$'\n",
    "def tokenize(sents, global_phonemizer, tokenizer, text_cleaner):\n",
    "    batched = []\n",
    "    max_id_length = 0\n",
    "    lengths = []\n",
    "    for sent in sents:\n",
    "        pretextcleaned = ' '.join(phonemize(sent, global_phonemizer, tokenizer)['phonemes'])\n",
    "        cleaned = text_cleaner(pretextcleaned)\n",
    "        batched.append(torch.LongTensor(cleaned))\n",
    "        max_id_length = max(max_id_length, len(cleaned))\n",
    "    phoneme_ids = torch.zeros((len(sents), max_id_length)).long()\n",
    "    mask = torch.zeros((len(sents), max_id_length)).float()\n",
    "    for i, c in enumerate(batched):\n",
    "        phoneme_ids[i,:len(c)] = c\n",
    "        mask[i,:len(c)] = 1\n",
    "    return phoneme_ids, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d0e904d-48b8-4f87-99a5-8de5f0dfef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pltbert_embs(s, global_phonemizer, tokenizer, text_cleaner):\n",
    "    \"\"\"\n",
    "    Input: list of texts\n",
    "\n",
    "    Output: output of pretrained Albert model - (batch_size, num_tokens, 768)\n",
    "    \"\"\"\n",
    "    phoneme_ids, attention_mask = tokenize(s, global_phonemizer, tokenizer, text_cleaner)\n",
    "    return plbert(phoneme_ids, attention_mask=attention_mask).last_hidden_state, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "37e7c8af-7923-432a-996f-09534ee7ef62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 116, 768])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pltbert_embs([text,text,text], global_phonemizer, tokenizer, text_cleaner).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b1de8-048b-4b03-a52e-fab804fdb1db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec851f85-4932-4a4b-9053-1b1e91133f36",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "613de146-2252-4577-84c6-05257d558bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperModel\n",
    "# from datasets import load_dataset\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "import librosa\n",
    "ref_waveform, sr = librosa.load(\"/Users/ajaybati/Downloads/0.23.0612.1.AT.PHL.alorica/results/originalTestFiles/maleb2.wav\", sr=16000)\n",
    "input_features = processor(ref_waveform, sampling_rate=16000, return_tensors=\"pt\").input_features \n",
    "\n",
    "input_features.shape\n",
    "\n",
    "decoder_input_ids = torch.tensor([[1,1]]) * model.config.decoder_start_token_id\n",
    "\n",
    "out = model(input_features=input_features, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "out['encoder_last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfaa78-c60c-4e60-974a-d2297e920fed",
   "metadata": {},
   "source": [
    "### Main Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5006795e-7af4-47f8-8d05-bbe7cfbfff4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-07-18 00:46:11 nemo_logging:349] /opt/conda/envs/sanas2/lib/python3.8/site-packages/whisper/timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "      def backtrace(trace: np.ndarray):\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"small.en\")\n",
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"miipherTestDataset/train/clean_trainset_wav/p234_003.wav\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91698571-b004-4b33-a6a7-156d4aa5040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.embed_audio(mel.reshape(1,*mel.shape).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0197702-9fe9-4a1b-9dce-7cee183456fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7dd6e6f-fa4e-4d0a-84c9-3e5c57d70bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelDimensions(n_mels=80, n_audio_ctx=1500, n_audio_state=768, n_audio_head=12, n_audio_layer=12, n_vocab=51864, n_text_ctx=448, n_text_state=768, n_text_head=12, n_text_layer=12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c50fcd-0f05-46c5-94a5-c947302bb189",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ECAPA/SpeakerNet Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a79526-2bf0-4737-987b-8a7fbb007a8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### NeMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee3254-9bd4-4b31-9837-1cf1d756cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models import EncDecSpeakerLabelModel\n",
    "speaker_encoder = EncDecSpeakerLabelModel.from_pretrained(model_name=\"speakerverification_speakernet\")\n",
    "ecapa = EncDecSpeakerLabelModel.from_pretrained(model_name='ecapa_tdnn')\n",
    "\n",
    "ecapa.eval()\n",
    "speaker_encoder.eval()\n",
    "\n",
    "def get_emb(model, wav_path):\n",
    "    return model.get_embedding(wav_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f1d81-f672-485a-8ec9-80d9bfa199c6",
   "metadata": {},
   "source": [
    "### JIT traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "293b59e9-2e14-48d6-b51e-9e9cbad645d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "ref_path = \"miipherTestDataset/train/clean_trainset_wav/p234_003.wav\"\n",
    "audio, sr = librosa.load(ref_path, sr=16000)\n",
    "audio = np.array([audio])\n",
    "audio_length = audio.shape[-1]\n",
    "audio_signal, audio_signal_len = (\n",
    "    torch.tensor(audio),\n",
    "    torch.tensor([audio_length])\n",
    ")\n",
    "kwarg = {'input_signal': audio_signal.cuda(), 'input_signal_length': audio_signal_len.cuda()} #batched input\n",
    "\n",
    "# spNet = torch.jit.load(\"/Users/ajaybati/Downloads/0.23.0612.1.AT.PHL.alorica/spNet_traced.jit\")\n",
    "ecapa = torch.jit.load(\"ecapa2_traced.jit\")\n",
    "\n",
    "ecapa(**kwarg)[-1].shape #logits, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b138f-892a-449e-bec2-e744ed0ba1a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conformer Blocks (loading 1 block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c52f830-d01b-4d5f-a5c2-027f895a3452",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2324367004.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 26\u001b[0;36m\u001b[0m\n\u001b[0;31m    encoder_dim: int = 512,\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from conformer.conformer.encoder import ConformerBlock\n",
    "\"\"\"\n",
    "Conformer block contains two Feed Forward modules sandwiching the Multi-Headed Self-Attention module\n",
    "and the Convolution module. This sandwich structure is inspired by Macaron-Net, which proposes replacing\n",
    "the original feed-forward layer in the Transformer block into two half-step feed-forward layers,\n",
    "one before the attention layer and one after.\n",
    "\n",
    "Args:\n",
    "    encoder_dim (int, optional): Dimension of conformer encoder\n",
    "    num_attention_heads (int, optional): Number of attention heads\n",
    "    feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n",
    "    conv_expansion_factor (int, optional): Expansion factor of conformer convolution module\n",
    "    feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n",
    "    attention_dropout_p (float, optional): Probability of attention module dropout\n",
    "    conv_dropout_p (float, optional): Probability of conformer convolution module dropout\n",
    "    conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n",
    "    half_step_residual (bool): Flag indication whether to use half step residual or not\n",
    "\n",
    "Inputs: inputs\n",
    "    - **inputs** (batch, time, dim): Tensor containing input vector\n",
    "\n",
    "Returns: outputs\n",
    "    - **outputs** (batch, time, dim): Tensor produces by conformer block.\n",
    "\"\"\"\n",
    "ConformerBlock(\n",
    "    encoder_dim: int = 512,\n",
    "    num_attention_heads: int = 8,\n",
    "    feed_forward_expansion_factor: int = 4,\n",
    "    conv_expansion_factor: int = 2,\n",
    "    feed_forward_dropout_p: float = 0.1,\n",
    "    attention_dropout_p: float = 0.1,\n",
    "    conv_dropout_p: float = 0.1,\n",
    "    conv_kernel_size: int = 31,\n",
    "    half_step_residual: bool = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ed297-fdbc-473e-95ce-cc2bda70a5c1",
   "metadata": {},
   "source": [
    "## Post-Net\n",
    "https://github.com/NVIDIA/tacotron2/blob/master/hparams.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d69b290f-b7f8-47c8-a2fc-ab27807e8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNorm(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
    "        super(ConvNorm, self).__init__()\n",
    "        if padding is None:\n",
    "            assert(kernel_size % 2 == 1)\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    padding=padding, dilation=dilation,\n",
    "                                    bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        conv_signal = self.conv(signal)\n",
    "        return conv_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4377b484-931a-401a-87c6-01d430eb5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class PostNet(nn.Module):\n",
    "    \"\"\"Postnet\n",
    "        - Five 1-d convolution with 512 channels and kernel size 5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(PostNet, self).__init__()\n",
    "        self.convolutions = nn.ModuleList()\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm(hparams.n_mel_channels, hparams.postnet_embedding_dim,\n",
    "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
    "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
    "                         dilation=1, w_init_gain='tanh'),\n",
    "                nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
    "        )\n",
    "\n",
    "        for i in range(1, hparams.postnet_n_convolutions - 1):\n",
    "            self.convolutions.append(\n",
    "                nn.Sequential(\n",
    "                    ConvNorm(hparams.postnet_embedding_dim,\n",
    "                             hparams.postnet_embedding_dim,\n",
    "                             kernel_size=hparams.postnet_kernel_size, stride=1,\n",
    "                             padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
    "                             dilation=1, w_init_gain='tanh'),\n",
    "                    nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
    "            )\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm(hparams.postnet_embedding_dim, hparams.n_mel_channels,\n",
    "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
    "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
    "                         dilation=1, w_init_gain='linear'),\n",
    "                nn.BatchNorm1d(hparams.n_mel_channels))\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.convolutions) - 1):\n",
    "            x = F.dropout(torch.tanh(self.convolutions[i](x)), 0.5, self.training)\n",
    "        x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4ed69-9ae1-46dc-9ad2-15bab11b7577",
   "metadata": {},
   "source": [
    "# Miipher2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe13d2d-c90e-42b0-a0f3-f34e1bd7b036",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74210654-ad09-4151-be2f-6c0ef5ba189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conformer.conformer.encoder import ConformerBlock\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c4c5eba-a8ee-4a6b-82df-42fc7c3d2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Miipher2(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_classes (int): Number of classification classes\n",
    "\n",
    "    Inputs: inputs, input_lengths\n",
    "        - plbertDim: feature dimension of pl-bert embeddings \n",
    "        - spEncDim: feature dimension of speaker encoder embeddings \n",
    "        - whisperLen: first dimension (time steps) of whisper embeddings\n",
    "        - whisperDim: feature dimension of whisper embeddings\n",
    "        - modelDim: hidden_size that most of the model operates on\n",
    "        - crossAttHeads: number of heads in cross attention layer\n",
    "        - conformerBlockSettings: Conformer Block hyper-parameters (look above in ConformerBlock section)\n",
    "        - hparams: PostNet hyper-parameters, look here: https://github.com/NVIDIA/tacotron2/blob/master/hparams.py\n",
    "\n",
    "    Returns: outputs, output_lengths\n",
    "        - **outputs**\n",
    "        - **output_lengths** \n",
    "    \"\"\"\n",
    "    def __init__(self, plbertDim, spEncDim, whisperLen, whisperDim, modelDim,\n",
    "                 crossAttHeads, crossAttDim, conformerBlockSettings, hparams, learning_rate=1e-4) -> None:\n",
    "        super(Miipher2, self).__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        #init 3 linear layers\n",
    "        self.plbert_lin = nn.Linear(plbertDim, modelDim)\n",
    "        self.whisper_lin = nn.Linear(whisperDim, modelDim)\n",
    "                         \n",
    "        #Stack 4 times:\n",
    "            #init cross attention module - input (128), hidden_size (512)\n",
    "            #init Conformer Block - attention -> input (128), hidden_size (512)\n",
    "            #init layer norm\n",
    "        self.cross_attention = nn.ModuleList([])\n",
    "        self.layer_norm = nn.ModuleList([])\n",
    "        self.conformerBlock = nn.ModuleList([])\n",
    "        for x in range(2): #changed 1->2\n",
    "            # nn.Linear(modelDim, crossAttDim), \n",
    "            # nn.Linear(modelDim, crossAttDim), \n",
    "            # nn.Linear(modelDim, crossAttDim), \n",
    "            self.cross_attention.append(nn.MultiheadAttention(modelDim, crossAttHeads, batch_first=True)) #changed->dropout=0\n",
    "            self.layer_norm.append(nn.LayerNorm([whisperLen, modelDim]))\n",
    "            self.conformerBlock.append(ConformerBlock(*conformerBlockSettings))\n",
    "\n",
    "        #make sure to reset hparams while loading it. input dimension to postnet is whisperDim => n_mel_channels=whisperDim. \n",
    "        #maybe change postnet_embedding_dim\n",
    "        self.postnet = PostNet(hparams)\n",
    "        self.whisper_proj = nn.Linear(modelDim, whisperDim)\n",
    "        self.layer_norm2 = nn.LayerNorm([whisperLen, modelDim])\n",
    "        #LOSS\n",
    "        self.l2_loss = nn.MSELoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, plbert_embs, spEnc_embs, whisper_embs, attention_mask):\n",
    "        \"\"\"\n",
    "        Inputs: plbert_embs, spEnc_embs, whisper_embs\n",
    "            - plbert_embs (Tensor[batch_size, num_tokens, hidden_size]): png-bert replacement; Phoneme-level embeddings from transcript\n",
    "            - spEnc_embs (Tensor[batch_size, hidden_size]): Speaker Encoder Embeddings (speakerNet/Ecapa)\n",
    "            - whisper_embs (Tensor[batch_size, seq_len, hidden_size]): w2v-bert replacement; Whisper encoder-level embeddings of audio\n",
    "    \n",
    "        Returns: outputs\n",
    "            - outputs (Tensor[batch_size, seq_len, hidden_size]): predicted, clean whisper-level embeddings\n",
    "        \"\"\"\n",
    "        #all linear output will be hidden_dim=modelDim\n",
    "        plbert_out = self.plbert_lin(plbert_embs) #(b, num_tokens, model_dim)\n",
    "\n",
    "        whisper_out = self.whisper_lin(whisper_embs)\n",
    "        \n",
    "        film1_out = plbert_out\n",
    "        for x in range(2): #changed 1->2\n",
    "            att = self.cross_attention[x] #need to do attention masking here\n",
    "            query, key, value = whisper_out, film1_out, film1_out\n",
    "            att_out, _ = att(query, key, value, key_padding_mask=(1-attention_mask))\n",
    "\n",
    "            whisper_out = whisper_out + att_out\n",
    "            layer_out = self.layer_norm[x](whisper_out)\n",
    "            conf_out = self.conformerBlock[x](layer_out)\n",
    "            whisper_out = whisper_out + conf_out\n",
    "\n",
    "        # self.layer_norm2(whisper_out)\n",
    "        whisper_out = self.whisper_proj(whisper_out)\n",
    "        whisper_permute = torch.permute(whisper_out, (0,2,1))\n",
    "        post_out = self.postnet(whisper_permute)\n",
    "        output = whisper_out + torch.permute(post_out, (0,2,1))\n",
    "        return output\n",
    "\n",
    "    def norm_loss(self, gt, preds):\n",
    "        \"\"\"\n",
    "        Inputs: gt, preds\n",
    "            - gt(Tensor[batch_size, seq_len, hidden_size])\n",
    "            - preds(Tensor[batch_size, seq_len, hidden_size])\n",
    "    \n",
    "        Returns: loss (reduction='mean')\n",
    "            - loss (Tensor[1]) = (1 norm + 2 norm + spectral convergence), reduced\n",
    "        \"\"\"\n",
    "        norm1 = self.l1_loss(preds, gt)\n",
    "        norm2 = self.l2_loss(preds, gt)\n",
    "        spectr = norm2/((gt**2).sum()/np.prod(list(gt.shape)))\n",
    "        loss = norm1 + norm2 + spectr\n",
    "        return loss, norm1, norm2, spectr\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        plbertembs, whisper_noisy, speakerembs, whisper_clean, att_mask = train_batch\n",
    "        logits = self.forward(plbertembs, speakerembs, whisper_noisy, att_mask)\n",
    "        loss, norm1, norm2, spectr = self.norm_loss(logits, whisper_clean)\n",
    "        # print(\"-\"*10+\"DEBUGGING\"+\"-\"*10)\n",
    "        # print(logits)\n",
    "        # print(\"*\"*20)\n",
    "        # print(whisper_clean)\n",
    "        # print(\"*\"*20)\n",
    "        # print(loss, norm1, norm2, spectr)\n",
    "        # print(\"-\"*10+\"DEBUGGING done\"+\"-\"*10)\n",
    "        self.log_dict(\n",
    "            {'train_loss':loss,\n",
    "             'train_norm1_loss':norm1,\n",
    "             'train_norm2_loss':norm2,\n",
    "             'train_spectral':spectr}, \n",
    "            on_step=True, \n",
    "            on_epoch=True, \n",
    "            prog_bar=True\n",
    "        )\n",
    "        return norm2 #changed from loss->norm2->loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        plbertembs, whisper_noisy, speakerembs, whisper_clean, att_mask = val_batch\n",
    "        logits = self.forward(plbertembs, speakerembs, whisper_noisy, att_mask)\n",
    "        loss, norm1, norm2, spectr = self.norm_loss(logits, whisper_clean)\n",
    "        self.log_dict(\n",
    "            {'test_loss':loss,\n",
    "             'test_norm1_loss':norm1,\n",
    "             'test_norm2_loss':norm2,\n",
    "             'test_spectral':spectr}, \n",
    "            on_step=True, \n",
    "            on_epoch=True, \n",
    "            prog_bar=True\n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "        # return {\"optimizer\": optimizer, \"lr_scheduler\": {\n",
    "        #     \"scheduler\": torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0.5, total_iters=7)}}\n",
    "\n",
    "\n",
    "    # def on_before_optimizer_step(self, optimizer, optimizer_idx):\n",
    "    #     # Compute the 2-norm for each layer\n",
    "    #     # If using mixed precision, the gradients are already unscaled here\n",
    "    #     norms = grad_norm(self.layer, norm_type=2)\n",
    "    #     self.log_dict(norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ef4d7-9982-4524-a09a-790abf8f0a53",
   "metadata": {},
   "source": [
    "### Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47867c54-2097-4393-ba50-df9715d64174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"plbert/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bae2953-2188-41cc-a749-0d507a3e6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hparams import create_hparams\n",
    "from hparams import HParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dde33f96-5a20-4f60-bd21-7b40a46a940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIM = 512 #changed from 256->512\n",
    "whisperLen = 1500\n",
    "whisper_dim = 768\n",
    "plbert_dim = 768\n",
    "speakernet_dim = 192\n",
    "crossAttHeads = 8 #changed from 16->8\n",
    "crossAttDim = 256\n",
    "\n",
    "hparams = create_hparams()\n",
    "hparams.n_mel_channels = whisper_dim\n",
    "hparams.postnet_embedding_dim = 512\n",
    "\n",
    "conformerBlockSettings = HParams(encoder_dim = MODEL_DIM,\n",
    "    num_attention_heads = 4, #changed from 8->4\n",
    "    feed_forward_expansion_factor = 2, #changed from 4->2\n",
    "    conv_expansion_factor = 2,\n",
    "    #removed all dropouts\n",
    "    feed_forward_dropout_p = 0,\n",
    "    attention_dropout_p = 0,\n",
    "    conv_dropout_p = 0,\n",
    "    conv_kernel_size = 7, #changed from 31->11->3->7\n",
    "    half_step_residual = True)\n",
    "conformerBlockSettings = conformerBlockSettings.tup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa5204-01f2-4f9d-af61-8919e4da54e6",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c215d0a-a2df-4c0b-a14f-54efd11c4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "miipher = Miipher2.load_from_checkpoint(\"checkpoints_test4iter3/epoch=79-step=461520-train_loss=0.04.ckpt\",\n",
    "                                        plbertDim=plbert_dim, \n",
    "                                        spEncDim=speakernet_dim, \n",
    "                                        whisperLen=whisperLen, \n",
    "                                        whisperDim=whisper_dim, \n",
    "                                        modelDim=MODEL_DIM,\n",
    "                                        crossAttHeads=crossAttHeads, \n",
    "                                        crossAttDim=crossAttDim, \n",
    "                                        conformerBlockSettings=conformerBlockSettings, \n",
    "                                        hparams=hparams)\n",
    "miipher.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e05f290-ece6-4d9e-802c-257ccad399a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-08-10 20:33:13 nemo_logging:349] /opt/conda/envs/sanas2/lib/python3.8/site-packages/whisper/timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "      def backtrace(trace: np.ndarray):\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "whispermodel = whisper.load_model(\"small.en\")\n",
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"miipherTestDataset/test/noisy_testset_wav/p232_104.wav\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio)\n",
    "a = whispermodel.embed_audio(mel.reshape(1,*mel.shape).cuda()).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21b96119-e083-4641-a7c3-473fa4cba616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vocoder = torch.jit.load(\"gs-vec2wav-base_19_July_23.jit\")\n",
    "spNet = torch.jit.load(\"spNet_traced.jit\")\n",
    "ecapa = torch.jit.load(\"ecapa_traced.jit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a31254c9-2fc3-4678-907f-981135641ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "def evaluate(model, text, wavPath):\n",
    "    #get plbert\n",
    "    plbert_embs, att_mask = get_pltbert_embs([text], global_phonemizer, tokenizer, text_cleaner)\n",
    "    #get whisper\n",
    "    wav, _ = librosa.load(wavPath, sr=16000, duration=20)\n",
    "    audio = whisper.pad_or_trim(wav)\n",
    "    \n",
    "    mel = whisper.log_mel_spectrogram(audio)\n",
    "    a = whispermodel.embed_audio(mel.reshape(1,*mel.shape).cuda()).detach().cpu()\n",
    "    \n",
    "    #get spenc\n",
    "    kwarg = {'input_signal': torch.tensor(audio).reshape((1,-1)), 'input_signal_length': torch.tensor([audio.shape[-1]])}\n",
    "    spnet_embs = spNet(**kwarg)[-1]\n",
    "    ecapa_embs = ecapa(**kwarg)[-1]\n",
    "    # ecapa_embs = torch.zeros(*ecapa_embs.shape)\n",
    "\n",
    "\n",
    "    return model(plbert_embs, ecapa_embs, a, att_mask), spnet_embs, a\n",
    "\n",
    "def convert(vocoder, spEnc, modelOut, path):\n",
    "    \n",
    "    out = vocoder(torch.permute(modelOut,(0,2,1)), spEnc.reshape((1,-1,1)))\n",
    "    torchaudio.save(path, out.squeeze(0), 16000)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "066ebd2d-3b60-48c0-9f18-f453008174e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi my name is dumbo\n"
     ]
    }
   ],
   "source": [
    "#\"miipherTestDataset/train/trainset_txt/p299_148.txt\"\n",
    "#\"miipherTestDataset/train/noisy_traainset_wav/p299_148.wav\"\n",
    "with open(\"unseen/sp07_exhibition_sn15.txt\", 'r') as f:\n",
    "    text = \"hi my name is dumbo\"\n",
    "print(text)\n",
    "wavPath = \"unseen/sp07_exhibition_sn15.wav\"\n",
    "\n",
    "clean_whisper_out, spnet_embs, noisy = evaluate(miipher, text, wavPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a70c8ade-36f2-4f5e-9444-3ec249a43e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(vocoder, spnet_embs, clean_whisper_out, \"vocoderTest3.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "09125652-0ad6-44d6-878a-f5786579db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, _ = librosa.load(wavPath.replace(\"noisy\",\"clean\"), sr=16000, duration=20)\n",
    "audio = whisper.pad_or_trim(wav)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio)\n",
    "actual = whispermodel.embed_audio(mel.reshape(1,*mel.shape).cuda()).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "56e7e166-74c2-475a-aa06-7c67416df47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3413, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miipher.norm_loss(actual, clean_whisper_out)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82553c96-78a1-4b53-b7be-8758c046d7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
